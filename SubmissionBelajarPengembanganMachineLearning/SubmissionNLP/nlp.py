# -*- coding: utf-8 -*-
"""nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qimYXAPzyR5cUkdbwrl85HfPAgzst29K
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.optimizers import Adam
from keras.regularizers import l2
import string
import pickle
import tensorflow as tf

# Baca data
data_emosi = pd.read_csv('Data/emotion_sentimen_dataset.csv')
data_emosi.info()

data_emosi.describe()

data_emosi.shape

data_emosi.columns

data_emosi.Emotion.value_counts()

# Filter data to remove rows with "boredom" emotion
data_emosi = data_emosi[data_emosi['Emotion'] != 'boredom']
max_data_per_emotion = 10000
data_emosi_new = data_emosi.groupby('Emotion').head(max_data_per_emotion)

# Simpan DataFrame yang telah difilter kembali ke file CSV
data_emosi_new.to_csv("Data/data_emosi_new.csv", index=False)
data_emosi_new.info()

data_emosi_new.describe()
data_emosi = data_emosi_new

# Drop baris dengan nilai yang hilang
data_emosi = data_emosi.dropna()

# Preprocessing Data
# Pisahkan kolom teks dan emosi sebagai fitur dan label
X = data_emosi['text']
y = data_emosi['Emotion']

# Preprocessing tambahan
# Menghapus tanda baca
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

# Lematisasi
def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    return ' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(text)])

# Menghapus angka
def remove_numbers(text):
    return ''.join([i for i in text if not i.isdigit()])

# Menghapus stopwords
stop_words = set(stopwords.words('english'))
def remove_stopwords(text):
    return ' '.join([word for word in word_tokenize(text) if word.lower() not in stop_words])

# Preprocessing teks
def preprocess_text(text):
    text = remove_punctuation(text)
    text = lemmatize_text(text)
    text = remove_numbers(text)
    text = remove_stopwords(text)
    return text

X = X.apply(preprocess_text)

# Tokenisasi teks setelah preprocessing tambahan
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
X_seq = tokenizer.texts_to_sequences(X)
X_pad = pad_sequences(X_seq)

# Encoding label
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Pembagian Data
X_train, X_val, y_train, y_val = train_test_split(X_pad, y_encoded, test_size=0.2, random_state=42)

# Model Building
num_classes = len(np.unique(y_train))  # Mengambil jumlah kelas dari data pelatihan
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X_pad.shape[1]))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2(0.001)))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

# Model Compilation
optimizer = Adam(learning_rate=0.001)
model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Callbacks
class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if logs.get('accuracy') > 0.95 and logs.get('val_accuracy') > 0.95:
            #self.model.stop_training = True
            print("\nThe accuracy of the training set and the validation set has reached > 95%!")

callbacks = myCallback()

# Training Model
history = model.fit(X_train, y_train, batch_size=64, epochs=10,
                    validation_data=(X_val, y_val), verbose=2, callbacks=[callbacks])

# Export label encoder
with open("Model/label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

# Export the trained model
model.save("Model/emotion_det.h5")

# Save the tokenizer
with open("Model/tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)
print("Model, tokenizer, dan label encoder telah disimpan di path /Model.")

# Evaluasi Model
_, train_acc = model.evaluate(X_train, y_train, verbose=0)
_, val_acc = model.evaluate(X_val, y_val, verbose=0)

print("Training Accuracy: {:.4f}".format(train_acc))
print("Validation Accuracy: {:.4f}".format(val_acc))

# Plot loss dan akurasi
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Fungsi untuk melakukan prediksi pada kalimat uji
def predict_emotion(test_sentence):
    test_sentence = preprocess_text(test_sentence)
    test_sequence = tokenizer.texts_to_sequences([test_sentence])
    test_sequence_pad = pad_sequences(test_sequence, maxlen=X_pad.shape[1])
    predicted_class = np.argmax(model.predict(test_sequence_pad), axis=-1)
    predicted_emotion = label_encoder.inverse_transform(predicted_class)[0]
    return predicted_emotion

# Prepare simple test sentences and their expected emotions
simple_test_sentences = [
    "I feel happy.",
    "I felt joyful.",
    "It makes me content.",
    "She is anger.",
    "I had so much fun."
]
simple_expected_emotions = ['happiness', 'fun', 'happiness', 'anger', 'fun']

# Prepare complex test sentences and their expected emotions
complex_test_sentences = [
    "I'm feeling a bit worried.",
    "I'm feeling very enthusiastic.",
    "I'm feeling extremely tired.",
    "My sister surprised me yesterday.",
    "I love you so much."
]
complex_expected_emotions = ['worry', 'enthusiasm', 'tired', 'surprise', 'love']

# Check predictions for simple test sentences
correct_predictions_simple = 0
for i, sentence in enumerate(simple_test_sentences):
    predicted_emotion = predict_emotion(sentence)
    if predicted_emotion == simple_expected_emotions[i]:
        correct_predictions_simple += 1
    else:
        print(f"Simple Test {i+1}: Expected emotion '{simple_expected_emotions[i]}' but got '{predicted_emotion}'")

# Check predictions for complex test sentences
correct_predictions_complex = 0
for i, sentence in enumerate(complex_test_sentences):
    predicted_emotion = predict_emotion(sentence)
    if predicted_emotion == complex_expected_emotions[i]:
        correct_predictions_complex += 1
    else:
        print(f"Complex Test {i+1}: Expected emotion '{complex_expected_emotions[i]}' but got '{predicted_emotion}'")

# Calculate accuracy for simple and complex test sentences
accuracy_simple = correct_predictions_simple / len(simple_test_sentences)
accuracy_complex = correct_predictions_complex / len(complex_test_sentences)

# Print result for simple test sentences
if accuracy_simple == 1.0:
    print("All simple tests passed")
else:
    print(f"Simple Test Accuracy: {accuracy_simple * 100:.2f}%")

# Print result for complex test sentences
if accuracy_complex == 1.0:
    print("All complex tests passed")
else:
    print(f"Complex Test Accuracy: {accuracy_complex * 100:.2f}%")

